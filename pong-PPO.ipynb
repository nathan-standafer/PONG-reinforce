{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JSAnimation in /home/nathan/anaconda3/envs/pytorch/lib/python3.7/site-packages (0.1)\n",
      "using device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "game_name = 'SpaceInvadersDeterministic-v4'\n",
    "#env = gym.make('PongDeterministic-v4')\n",
    "env = gym.make(game_name)\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        ########\n",
    "        ## \n",
    "        ## Modify your neural network\n",
    "        ##\n",
    "        ########\n",
    "        \n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ########\n",
    "        ## \n",
    "        ## Modify your neural network\n",
    "        ##\n",
    "        ########\n",
    "    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "\n",
    "# run your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=600) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(what I call scalar function is the same as policy_loss up to a negative sign)\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    #print(\"rewards: {}\".format(rewards)) \n",
    "    \n",
    "    #calculate future rewards\n",
    "    future_rewards = []\n",
    "    for i in range(len(rewards)):\n",
    "        this_reward = np.zeros(rewards[0].shape)\n",
    "        for ii in range(i, len(rewards)):\n",
    "            this_reward += rewards[ii]*discount**(ii-i)\n",
    "        future_rewards.append(this_reward)\n",
    "    #print(\"future_rewards: {}\".format(future_rewards))    \n",
    "    \n",
    "    #normalize the rewards\n",
    "    mean = np.mean(future_rewards, axis=1)\n",
    "    #print(\"mean: {}\".format(mean))\n",
    "    std = np.std(future_rewards, axis=1) + 1.0e-10\n",
    "    #print(\"std: {}\".format(std))\n",
    "    future_rewards_normalized_np = (future_rewards - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "\n",
    "    #convert to tensors\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    future_rewards_normalized_tensor = torch.tensor(future_rewards_normalized_np, dtype=torch.float, device=device)\n",
    "    #print(\"future_rewards_normalized: {}\".format(future_rewards_normalized))\n",
    "    \n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    ratio = new_probs/old_probs\n",
    "    ratio_clipped = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    ratio_ppo = torch.min(ratio,ratio_clipped)\n",
    "    \n",
    "#     print(\"ratio.shape: {}\".format(ratio.shape))\n",
    "#     print(\"ratio_clipped.shape: {}\".format(ratio_clipped.shape))\n",
    "#     print(\"ratio_ppo.shape: {}\".format(ratio_ppo.shape))\n",
    "    \n",
    "#     print(\"ratio[:5]: {}\".format(ratio[:5]))\n",
    "#     print(\"ratio_clipped[:5]: {}\".format(ratio_clipped[:5]))\n",
    "#     print(\"ratio_ppo[:5]: {}\".format(ratio_ppo[:5]))\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    \n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+(1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    \n",
    "    return torch.mean(ratio_ppo*future_rewards_normalized_tensor + beta*entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar in /home/nathan/anaconda3/envs/pytorch/lib/python3.7/site-packages (2.5)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                          | ETA:  0:22:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: 201.875000\n",
      "[195. 230. 455. 195.  80. 190. 140. 130.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                        | ETA:  0:21:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: 94.375000\n",
      "[105. 105.  75. 105. 105.  50. 105. 105.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  0:19:32\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: 83.125000\n",
      "[ 45. 130.  85. 130.  85.  65.  75.  50.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |######                                     | ETA:  0:18:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, score: 81.250000\n",
      "[ 45.  90.  55. 130.  50.  60.  90. 130.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  0:17:08\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, score: 101.250000\n",
      "[ 45.  70. 140. 125. 125.  85.  65. 155.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |##########                                 | ETA:  0:16:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120, score: 186.875000\n",
      "[155.  75. 105. 105. 360. 410. 105. 180.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |############                               | ETA:  0:15:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140, score: 175.625000\n",
      "[100. 120. 110. 155. 125. 185. 410. 200.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                              | ETA:  0:14:36\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 160, score: 78.750000\n",
      "[ 65.  60. 125. 100. 120.  45.  95.  20.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |###############                            | ETA:  0:13:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, score: 171.875000\n",
      "[135. 140. 165. 150. 195. 240. 180. 170.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:12:38\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, score: 106.250000\n",
      "[100.  30. 125. 125. 105. 105. 155. 105.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  44% |##################                         | ETA:  0:11:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 220, score: 98.750000\n",
      "[ 65. 125.  75.  95. 135. 120. 115.  60.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  48% |####################                       | ETA:  0:10:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 240, score: 94.375000\n",
      "[ 50. 105. 140.  80. 105.  55. 150.  70.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  52% |######################                     | ETA:  0:09:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 260, score: 120.000000\n",
      "[115. 140. 115. 135. 120. 100.  45. 190.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  56% |########################                   | ETA:  0:09:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280, score: 173.750000\n",
      "[220. 135. 200. 150. 190. 185. 125. 185.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |#########################                  | ETA:  0:08:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, score: 185.000000\n",
      "[135. 210. 170. 410. 135. 155. 165. 100.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  64% |###########################                | ETA:  0:07:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 320, score: 93.750000\n",
      "[160.  65. 135. 145.  90.  55.  25.  75.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  68% |#############################              | ETA:  0:06:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 340, score: 94.375000\n",
      "[105.  80. 110.  60. 140.  45. 115. 100.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  72% |##############################             | ETA:  0:05:39\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 360, score: 107.500000\n",
      "[105. 110. 140. 130. 170.  70.  60.  75.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  76% |################################           | ETA:  0:04:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 380, score: 109.375000\n",
      "[110.  70. 295. 120.  55.  90.  85.  50.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  80% |##################################         | ETA:  0:03:59\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, score: 64.375000\n",
      "[100. 100.  45.  60.  15.  65.  95.  35.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  84% |####################################       | ETA:  0:03:10\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 420, score: 98.750000\n",
      "[ 45.  70. 100. 160. 110. 130.  50. 125.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  88% |#####################################      | ETA:  0:02:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 440, score: 79.375000\n",
      "[ 60. 125.  85.  70.  70. 110.  35.  80.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  92% |#######################################    | ETA:  0:01:34\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 460, score: 65.625000\n",
      "[ 50. 100.  80.  55.  30. 130.  30.  50.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  96% |#########################################  | ETA:  0:00:47\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 480, score: 105.000000\n",
      "[105. 105. 105. 105. 105. 105. 105. 105.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 0:20:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, score: 105.000000\n",
      "[105. 105. 105. 105. 105. 105. 105. 105.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv(game_name, n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 600\n",
    "SGD_epoch = 9\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "#         L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "#                                           epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO_{}.policy'.format(game_name))\n",
    "\n",
    "# load policy if needed\n",
    "# policy = torch.load('PPO.policy')\n",
    "\n",
    "# try and test out the solution \n",
    "# make sure GPU is enabled, otherwise loading will fail\n",
    "# (the PPO verion can win more often than not)!\n",
    "#\n",
    "# policy_solution = torch.load('PPO_solution.policy')\n",
    "# pong_utils.play(env, policy_solution, time=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
